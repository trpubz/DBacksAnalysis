# AZ DBacks ML Analysis
This is a CRISP-DM project aimed at exploring Arizona Diamondback lineup optimizations and matchup analyses.  This project satisfied a requirement from The Air Force Institute of Technology with the goal to perform a neural network analysis on a problem of interest, and compare its performance to a classical modeling approach.  

## Business Understanding
    “There are rich teams and there are poor teams. Then, there’s 50 feet of crap. And then there’s us.” – Billy Beane
The Arizona Diamondbacks are slightly more valuable than the Oakland Athletics but are indeed the least valuable franchise in the NL West (per Statista market research [1]).  Even with a decrease of intra-division games (from 76 down to 52) for the 2023 season, a third of the games are still played against division rivals.  Constructing and optimizing a depth chart around frequently encountered opponents using machine learning (ML), has at a minimum, a chance to increase division series wins, and a best-case scenario of improving playoff-chase leverage.
Sabermetrics were popularized in the 1980s by Bill James’s iconic work known as his Pythagorean Expected Win Ratio and then made mainstream by the book & movie Moneyball.  Sabermetrics can be described as statistical analysis beyond the rote descriptive statistics [2] and combines simple observations to generate more meaningful insight.  Applying ML to Sabermetrics has the opportunity to identify and exploit extremely powerful patterns.
Tom Tango & co.’s book, The Book, is a comprehensive review of Sabermetrics that dives into the field of predictive analysis.  He claims attempting to estimate a player’s true talent is akin to making a prediction [3].  The primary methodology this project will focus on is regression modeling.  In the context of this project, expected weighted on-base average (xwOBA) will be monitored to explore if there is a significant relationship between a pitch’s metrics/observations and a particular player’s batted ball outcome.  This preliminary analysis will lay the groundwork for a future, more holistic, analysis that expands the observation pool to include more pitchers and hitters after performing unsupervised machine learning where  pitchers will be grouped together into architypes based on various Statcast metrics, similar to The Book’s, la famiglias—Italian for families.  Certain players will be grouped together based on observed and statistical characteristics and then performance can be evaluated against a family which reduces variation implicit to small sample sizes.
The overarching hypothesis this project will explore is the hypothesis that not all hitters perform the same against all types of pitchers (e.g. lefty, soft-contact, ground-ball, good-command).  Tom Tango refutes the ability to draw this type of conclusive analysis and implies regression towards the mean is as sure as death and taxes.  This means that if a batter dominates against a family of pitchers for as many as 60 plate appearances (PAs), the likelihood that his next 60 PAs are closer to his average, is far more likely than his previous PAs against that family.  His conclusions are based on observations of past occurrences but does not adjust for year over year increases/decreases in ability and skill.  This is an analysis problem most suited for ML.
This project attempts to answer the questions:
•	Using Sabermetrics and MLB Statcast data, how well can classical ML and neural networks (NN), predict a player’s divisional statistics; specifically, xwOBA?  This stat is explained further in the Data Understanding section.
•	Are there other players around the league that could thrive in our test environment? [for a future research effort].
The methodology for this project will utilize a mélange of Statcast categorical and numerical data that will group various pitch metrics (pitch type, speed, & movement) and try to predict the outcomes.  A naïve model (past four season’s performance) will provide a baseline, then classical regression models will attempt to show improvement, then NN models will attempt to improve predictive ability beyond the classical models without overfitting the data.  While not in the scope of this project, a recurrent NN (RNN) has the potential to exploit the linear/time-series nature of a baseball season to further predict performance within hot and cold streaks.  According to the great Yogi Berra, “Baseball is 90% mental; the other half is physical”.  While there is no Sabermetric that can quantify a player’s mental state, a RNN can potentially identify signs of a streak.
This project will succeed if one or more ML models can predict player performance better than a naïve model.  Success will also be counted if the ideas contained in this project spurn enhancements and creativity to the discipline sports ML.
A comprehensive analysis, useful for a front office, would provide detailed models for each rostered player against all divisional opponents (batters vs. pitchers) and similar opponent profiles.  This paper will demonstrate the capabilities of ML with only one player (Ketel Marte, 2B/CF) vs. National League West pitching staffs.  2019-2022 data will be used for this analysis.

## Data Understanding
A moderate fidelity model will require pitch type, spin rate, and velocity among others.  These features are the underlying quantifiable metrics suitable for ML modeling insights.  The combination of such metrics is what can be classified in laymen’s terms as a rising fastball vs low-90s, or a sharp curve vs looping curve, or a circle change vs straight change.  The model will use those characteristics to predict a player’s batted ball outcome.  
This study will use xwOBA as the primary response variable.  This metric synthesizes two, more commonly used but incomplete stats—on-base percentage (OBP) and slugging percentage (SLG).   xwOBA was chosen as the output metric for its wide-recognition in the Sabermetrics community for being one of the most important and comprehensive offensive statistics.   xwOBA differs from wOBA in that it uses the launch speed and launch angle as the referential measures to provide a probability of similarly hit balls.  This removes defensive variations.  Additionally, wOBA is easily converted to weighted Runs Above Average (wRAA) which is a scalar alternative to wOBA, allowing for quick comparisons against league average.  For a full explanation, visit the Statcast glossary [8].

## Data Preparation
The data were preprocessed by filtering out sacrifice events (sac-bunt, sac-fly); only 0.2% of PAs.  An additional column labeled ‘pfx_v’ was computed as the Euclidean distance or vector magnitude of the pitch’s horizontal and vertical movement.  Additionally, any NaN occurrences in response variable was converted to the event’s ‘woba_value’.  Strikeouts and walks produce a NaN xwOBA because there is no ‘observed’ batted ball event; a strikeout has a theoretical (wOBA equivalent) xwOBA of 0.0 and a walk of 0.7—however a player’s ability to lay-off a ball is not likely correlated to that pitch’s metrics but rather the player’s plate discipline.  For this assumption, a total of 62 walks (7.2% PAs) were dropped from the data set.  Likewise, a strikeout-looking may not be completely dependent on a pitch’s metrics.  The hitter may have legitimately thought it to be outside the zone or the pitch sequence was perfectly executed to where the hitter was “froze”.  These associated outcomes do not explain a hitter’s ability to make contact or explain the resulting quality of said contact.  Additionally, permitting the remaining swinging strikeouts to remain in the data would potentially force the relationship to be analyzed as a classification problem as opposed to a regression problem due to the percentage of strikeouts in the dataset.  For these reasons, all 124 strikeout events (14.3% PAs)
This data set combines numerical and categorical features.  Categorical features were converted non-ordinally, using pandas.get_dummies().  Categorical features include ‘pitch_type’, ‘zone’, & ‘p_throws’.  Spin axis could also be considered a categorical feature and could potentially reveal additional relationships in future analyses.  The numerical features were normalized and scaled using sklearn.preprocessing PowerTransformer.  xwOBA, the target variable, was not normalized despite the log-normal shape to the data.  Residual analysis showed relatively strong heteroskedasticity making normalization unnecessary.

## Modeling
B.	Metrics
For this analysis, the regression metrics of root mean squared error (RMSE), R2 stat, and Akaike information criterion (AIC) were used to measure the performance of the models.  Minimizing the RMSE was the primary focus as it provides a scale for how far away the errors in prediction are from the true values.  RMSE is more sensitive (than MAE) to outliers and since the target variable is bounded by a theoretical min/max, outliers are not a concern [9].  R2 is useful in that it explains the proportion of variance accounted for in a model [10].  Lastly, AIC is the last metric used to relatively compare models as it is good at penalizing models with more parameters and rewards the models with better fits [9].
C.	Naïve Model
For regular season games from 2019—2022, Ketel Marte’s xwOBA for all PAs without walks or strikeouts (675) is .365.  This value was used as the predictor for the PAs against NL West opponents.  The RMSE equaled .366 telling us the naïve predictor was as good as a random guess at the desired target.  An R2 score of -0.0004 indicates practical mean-equivalent predictability.
D.	Classical Modeling  
The data was split into a 65/35 train/test segmentation.  The model was trained on the larger training data subset and then used to predict the output on the small test subset.  The true data was compared to the predicated data which generated the various scoring metrics for the models.
The first model created was a standard ordinary least squares (OLS) regression with the full subset of features.  Of note, the p-values for features with presupposed influence such as velo and pitcher handedness indicated weak influence.  The full-featured model produced no improvement over the naïve model.  The second model employed a backward selection methodology by evaluating the p-val stats of the features.  After removing features with a p-value < 0.10 in a piecewise manner and remodeling the new feature selections, the only remaining features were interestingly the Sinker, Curveball, & two out-of-zone zones.  This model performed slightly better than the full-featured model.  For the third model, a recursive feature elimination (RFE) model was ran over all ~30 features.  The best model was nearly identical to the backward selection model.  A fourth model, LassoLARSIC (Information Criterion) model was ran.  Least-angle regression (LARS) is a regression algorithm for high-dimensional data performed similar to stepwise forward-selection.  This method penalized coefficients until the lowest AIC was identified.  Lastly, a model containing only fastballs was generated.  This model performed the best in every scoring metric.
E.	Neural Network Modeling
Since predicting xwOBA is naturally a regression problem, the loss function chosen was mean_squared_error.  This allows for incidental comparison to the previous regression models.  The Adam optimizer algorithm was chosen for its general applicability and widespread utilization for similar regression problems.  The NN models used initial normalization by utilizing z-scaled activations at the first layer with the intent to prevent variance shifting during weight assignments.  The problem-set suggests a multi-layer perceptron approach (as opposed to a recurrent neural net RNN) and thus lends itself to the ReLU activation function.  Additionally, the response variable’s lognormal shape and the presence of heterogeneous categorical features sparked the need to utilize a sigmoid function in a different layer.  And because this is a traditional regression problem, the NN’s used a linear activation function for the output layer.
The same train/test split used in the classical modeling was used for the NN analysis.  For the higher fidelity models, utilizing the tensorflow callback EarlyStopping class allowed the lowest validation loss value to identify the best model parameters to minimize overfitting problems.
A hyperparameter sweep consisting of neurons, layers, batch size, & epochs was conducted.  An L1+L2 ElasticNet regularization method was applied to the best model with notable improvement.

## Evaluation
A.	Classical Modeling  
Overall, performance was generally weak with marginally better R2 scores for the four various supervised learning models.  The quantifiable improvements are observed by focusing on the RMSE.  The best model improved the average predicted distance from the true value up to 9.6%.  Using RMSE as the valued metric: the Fastball Only Regression model performed the best.
B.	Neural Network Modeling
A 1-D simple NN performed surprising well compared to the naïve model.  The chosen feature was release_spin_rate to predict xwOBA.  The next model tuning required performing a sweep of neurons, layers, batch size, and number of epochs produced nearly identical results to other models.  Additionally, A model was tuned with a kernel regulation, the  L1L2. This regularization penalty term is the sum of squared coefficients.  This drives coefficients to be smaller and since the moderately large number of features, NetElastic is an appropriate regularization.  Surprisingly the regularized model performed no better than the single feature model.  Lastly, like the classical regression, the data was filtered to only include fastballs; this model was by far the best NN model performing 8.2% better than the naïve model.

## Deployment
