{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Prep & Modeling\n",
    "## Filtering"
   ],
   "metadata": {
    "collapsed": false,
    "id": "3m83__aSeqPA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-set shape prior to filtering: (867, 29)\n",
      "data-set shape after to filtering sac hits, walks, strikeouts: (681, 29)\n",
      "data-set shape after to filtering missing spin observations: (675, 29)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "marte_raw = pd.read_csv('marte_vs_nlwest_19_22.csv',\n",
    "                        usecols=['pitch_type', 'game_date', 'release_speed', 'release_pos_x', 'release_pos_z', 'pitcher', 'zone', 'des', 'p_throws', 'bb_type', 'balls', 'strikes', 'game_year', 'pfx_x', 'pfx_z', 'hc_x', 'hc_y', 'hit_distance_sc', 'launch_speed', 'launch_angle', 'effective_speed', 'release_spin_rate', 'release_extension', 'estimated_woba_using_speedangle', 'woba_value', 'woba_denom', 'iso_value', 'launch_speed_angle', 'pitch_name'])\n",
    "\n",
    "# rename the primary response variable\n",
    "marte_raw.rename(columns={'estimated_woba_using_speedangle': 'xwOBA'}, inplace=True)\n",
    "print(f\"data-set shape prior to filtering: {marte_raw.shape}\")\n",
    "\n",
    "# filter sacrifice events; if woba_denom is NaN, then sacrifice event\n",
    "# filter walks and strikeout events\n",
    "marte_raw.dropna(axis='rows', subset=['woba_denom', 'xwOBA'], how='any', inplace=True)\n",
    "print(f\"data-set shape after to filtering sac hits, walks, strikeouts: {marte_raw.shape}\")\n",
    "\n",
    "# filter missing observation data\n",
    "marte_raw.dropna(axis='rows', subset=['release_spin_rate'], how='any', inplace=True)\n",
    "print(f\"data-set shape after to filtering missing spin observations: {marte_raw.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjpJVOHZeqPD",
    "outputId": "8a3f1122-c3ed-40be-ede5-aad90efd49bf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- release_speed\n",
    "    - Pitch velocities from 2008-16 are via Pitch F/X, and adjusted to roughly out-of-hand release point. All velocities from 2017 and beyond are Statcast, which are reported out-of-hand.\n",
    "- release_pos_x\n",
    "    - horizontal Release Position of the ball measured in feet from the catcher's perspective.\n",
    "- release_pos_z\n",
    "    - Vertical Release Position of the ball measured in feet from the catcher's perspective.\n",
    "- plate_x\n",
    "    - Horizontal position of the ball when it crosses home plate from the catcher's perspective.\n",
    "- plate_z\n",
    "    - Vertical position of the ball when it crosses home plate from the catcher's perspective.\n",
    "- pitcher\n",
    "    - MLB Player Id tied to the play event.\n",
    "- zone\n",
    "    - Zone location of the ball when it crosses the plate from the catcher's perspective.\n",
    "- pfx_x\n",
    "    - Horizontal movement in feet from the catcher's perspective.\n",
    "- pfx_z\n",
    "    - Vertical movement in feet from the catcher's perpsective.\n",
    "- effective_speed\n",
    "    - Derived speed based on the the extension of the pitcher's release.\n",
    "- release_spin\n",
    "    - Spin rate of pitch tracked by Statcast.\n",
    "- spin_axis\n",
    "    - The Spin Axis in the 2D X-Z plane in degrees from 0 to 360, such that 180 represents a pure backspin fastball and 0 degrees represents a pure topspin (12-6) curveball"
   ],
   "metadata": {
    "collapsed": false,
    "id": "LFY3TD-xeqPF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering ext., Computed Features, NaN Conversions"
   ],
   "metadata": {
    "collapsed": false,
    "id": "wd6jzfWWeqPF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new data-set shape after to filtering for proper metrics: (675, 8)\n",
      "number of NaN xwOBA PAs prior to conversion: 0\n",
      "   xwOBA p_throws pitch_type  release_speed  release_spin_rate  zone  pfx_x  \\\n",
      "0  0.709        R         SI           92.9             2303.0     5  -1.15   \n",
      "1  0.262        R         SI           92.6             2316.0     9  -1.05   \n",
      "2  0.231        R         SI           93.0             2245.0     8  -1.13   \n",
      "3  0.117        L         SI           93.3             2189.0     8   1.19   \n",
      "4  0.438        R         SI           94.1             2126.0     4  -1.34   \n",
      "\n",
      "   pfx_z     pfx_v  \n",
      "0   0.68  1.336001  \n",
      "1   0.80  1.320038  \n",
      "2   0.79  1.378768  \n",
      "3   0.28  1.222497  \n",
      "4   0.45  1.413542  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# filter raw set with proposed model metrics\n",
    "marte_vs = marte_raw.drop(columns=['woba_value','game_year', 'pitcher', 'game_date', 'release_pos_x', 'release_pos_z', 'des', 'bb_type', 'balls', 'strikes', 'hc_x', 'hc_y', 'hit_distance_sc', 'launch_speed', 'launch_angle', 'effective_speed', 'release_extension', 'woba_denom', 'iso_value', 'launch_speed_angle', 'pitch_name'])\n",
    "print(f\"new data-set shape after to filtering for proper metrics: {marte_vs.shape}\")\n",
    "\n",
    "# vectorize the horizontal & vertical movement using pythagorean theorem\n",
    "marte_vs['pfx_v'] = np.sqrt(marte_vs.pfx_x ** 2 + marte_vs.pfx_z ** 2)\n",
    "\n",
    "print(f\"number of NaN xwOBA PAs prior to conversion: {marte_vs.xwOBA.isnull().sum()}\")\n",
    "\n",
    "# if you do not filter walks and strikeouts from the raw data, this will allow more granular filtering\n",
    "################################################################\n",
    "# # if the xwOBA stat is NaN & a strikeout, then need to use the woba_value\n",
    "# marte_vs['xwOBA'] = marte_vs.apply(lambda x: x.woba_value if (math.isnan(x.xwOBA) and x.woba_value == 0.0) else x.xwOBA, axis=1)\n",
    "#\n",
    "# # filter out walks, dropped 54 walks\n",
    "# marte_vs.dropna(axis='rows', subset=['xwOBA'], how='any', inplace=True)\n",
    "# print(f\"data-set shape after to filtering walks: {marte_vs.shape}\")\n",
    "##########################################################\n",
    "\n",
    "# reorder columns for logical readability\n",
    "marte_vs = marte_vs.reindex(columns=['xwOBA', 'p_throws', 'pitch_type', 'release_speed', 'release_spin_rate', 'zone', 'pfx_x', 'pfx_z', 'pfx_v'])\n",
    "\n",
    "print(marte_vs.head())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXIpP4T_eqPF",
    "outputId": "836785a0-8c81-47e6-b56e-ae3c079fdb0d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformations\n",
    "### Dummies\n",
    "A common mistake is to apply transformations to the entire data before splitting into training and test sets. This will bias the model evaluation because information would have leaked from the test set to the training set."
   ],
   "metadata": {
    "collapsed": false,
    "id": "k8-b34Z7eqPJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   xwOBA  release_speed  release_spin_rate  pfx_x  pfx_z     pfx_v  \\\n0  0.709           92.9             2303.0  -1.15   0.68  1.336001   \n1  0.262           92.6             2316.0  -1.05   0.80  1.320038   \n2  0.231           93.0             2245.0  -1.13   0.79  1.378768   \n3  0.117           93.3             2189.0   1.19   0.28  1.222497   \n4  0.438           94.1             2126.0  -1.34   0.45  1.413542   \n\n   pitch_type_CH  pitch_type_CU  pitch_type_FC  pitch_type_FF  ...  zone_5  \\\n0              0              0              0              0  ...       1   \n1              0              0              0              0  ...       0   \n2              0              0              0              0  ...       0   \n3              0              0              0              0  ...       0   \n4              0              0              0              0  ...       0   \n\n   zone_6  zone_7  zone_8  zone_9  zone_11  zone_12  zone_13  zone_14  \\\n0       0       0       0       0        0        0        0        0   \n1       0       0       0       1        0        0        0        0   \n2       0       0       1       0        0        0        0        0   \n3       0       0       1       0        0        0        0        0   \n4       0       0       0       0        0        0        0        0   \n\n   p_throws_R  \n0           1  \n1           1  \n2           1  \n3           0  \n4           1  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>xwOBA</th>\n      <th>release_speed</th>\n      <th>release_spin_rate</th>\n      <th>pfx_x</th>\n      <th>pfx_z</th>\n      <th>pfx_v</th>\n      <th>pitch_type_CH</th>\n      <th>pitch_type_CU</th>\n      <th>pitch_type_FC</th>\n      <th>pitch_type_FF</th>\n      <th>...</th>\n      <th>zone_5</th>\n      <th>zone_6</th>\n      <th>zone_7</th>\n      <th>zone_8</th>\n      <th>zone_9</th>\n      <th>zone_11</th>\n      <th>zone_12</th>\n      <th>zone_13</th>\n      <th>zone_14</th>\n      <th>p_throws_R</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.709</td>\n      <td>92.9</td>\n      <td>2303.0</td>\n      <td>-1.15</td>\n      <td>0.68</td>\n      <td>1.336001</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.262</td>\n      <td>92.6</td>\n      <td>2316.0</td>\n      <td>-1.05</td>\n      <td>0.80</td>\n      <td>1.320038</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.231</td>\n      <td>93.0</td>\n      <td>2245.0</td>\n      <td>-1.13</td>\n      <td>0.79</td>\n      <td>1.378768</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.117</td>\n      <td>93.3</td>\n      <td>2189.0</td>\n      <td>1.19</td>\n      <td>0.28</td>\n      <td>1.222497</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.438</td>\n      <td>94.1</td>\n      <td>2126.0</td>\n      <td>-1.34</td>\n      <td>0.45</td>\n      <td>1.413542</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 28 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marte_vs_trx = pd.get_dummies(data=marte_vs, columns=['pitch_type', 'zone'])\n",
    "marte_vs_trx = pd.get_dummies(data=marte_vs_trx, columns=['p_throws'], drop_first=True)\n",
    "marte_vs_trx.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "84OYs0UjeqPJ",
    "outputId": "c585b648-7d5f-4f37-8af4-6d16a33516c9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature and output transformations"
   ],
   "metadata": {
    "collapsed": false,
    "id": "eOHPujHyeqPJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape\n",
      " (438, 28) (438,)\n",
      "Test set shape\n",
      " (237, 28) (237,)\n",
      "---\n",
      "Train trx set shape\n",
      " (438, 5) (438,)\n",
      "Test trx set shape\n",
      " (237, 5) (237,)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = marte_vs_trx.drop(columns=['xwOBA'])  # remove response var and extra features, spin_axis should be converted to categorical if desired to use in analysis, removed earlier on ingest\n",
    "\n",
    "y = marte_vs.xwOBA\n",
    "# split data into training and testing sets\n",
    "\n",
    "X = sm.add_constant(X)   # only needed for sm (not smf)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=1642)\n",
    "\n",
    "print('Train set shape\\n', X_train.shape, y_train.shape)\n",
    "print('Test set shape\\n', X_test.shape, y_test.shape)\n",
    "print('---')\n",
    "\n",
    "# transform numerical data\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
    "\n",
    "num_feats = ['release_speed', 'release_spin_rate', 'pfx_v', 'pfx_x', 'pfx_z']\n",
    "# print(X[num_feats].describe())\n",
    "# used for positive values\n",
    "pt = PowerTransformer()\n",
    "# get a transformed series of numerical features\n",
    "X_train_trx = pt.fit_transform(X_train[num_feats])\n",
    "# overwrite raw numerical data\n",
    "X_train[num_feats] = X_train_trx\n",
    "\n",
    "# transform the test set with the trained lambdas\n",
    "X_test_trx = pt.transform(X_test[num_feats])\n",
    "# overwrite raw numerical dataframe\n",
    "X_test[num_feats] = X_test_trx\n",
    "############################\n",
    "# # normalize output\n",
    "# mms = MinMaxScaler().fit(y_train.to_numpy().reshape(-1, 1))\n",
    "# y_train_trx = pd.Series(map(lambda x: x[0], mms.transform(y_train.to_numpy().reshape(-1, 1))), index=y_train.index)\n",
    "# # normalize on the test output, converted to a pandas.Series with the original indicies\n",
    "# y_test_trx = pd.Series(map(lambda x: x[0], mms.transform(y_test.to_numpy().reshape(-1, 1))), index=y_test.index)\n",
    "\n",
    "print('Train trx set shape\\n', X_train_trx.shape, y_train.shape)\n",
    "print('Test trx set shape\\n', X_test_trx.shape, y_test.shape)\n",
    "print('---')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ua5JsML8eqPJ",
    "outputId": "042f0b6e-7ecb-49aa-d3a9-1a0554da0670"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling\n",
    "### Naive Model\n",
    "*Note: did not transform the output for the naive model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "8IiTdci1eqPK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36489185185185186\n",
      "naive mse: 0.36604190443047196\n",
      "naive r2 score: -0.0003866610950109539\n"
     ]
    }
   ],
   "source": [
    "# Marte's xwOBA for last 4 seasons ('19-'22) == .348 || without walks xwOBA == .305\n",
    "from sklearn.metrics import *\n",
    "\n",
    "print(y.mean())\n",
    "y_same_xwOBA = pd.Series(.365, index=range(0, y_train.size))\n",
    "# if squared is false then rmse\n",
    "naive_rmse = mean_squared_error(y_true=y_train, y_pred=y_same_xwOBA, squared=False)\n",
    "print(f\"naive mse: {naive_rmse}\")\n",
    "\n",
    "naive_r2 = r2_score(y_true=y_train, y_pred=y_same_xwOBA)\n",
    "print(f\"naive r2 score: {naive_r2}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvUQkDpNeqPK",
    "outputId": "f156edcb-0b57-4917-f72d-fd9beeca1577"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NN\n",
    "#### Model 1"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ZwwZvAkgeqPM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "\n",
    "\n",
    "\n",
    "# normalizer = preprocessing.Normalization()\n",
    "# normalizer.adapt(np.array(train_features))\n",
    "X_1 = X_train.release_spin_rate\n",
    "X_1_test = X_test.release_spin_rate\n",
    "number_of_inputs = 1  # number of features\n",
    "model = keras.Sequential([layers.Dense(3, activation='relu', input_dim=number_of_inputs),\n",
    "                          layers.Dense(3, activation='relu'),\n",
    "                          layers.Dense(1, activation='linear') ]) # output layer for regression\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), RSquare()])\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_1,\n",
    "    y_train,\n",
    "    batch_size=42,\n",
    "    epochs=100,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(X_1_test, y_test),\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_1_test, y_test, batch_size=30)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "print(\"Generate predictions for test data\")\n",
    "predictions = model.predict(X_1_test)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "X1_rmse = mean_squared_error(y_true=y_test, y_pred=predictions, squared=False)\n",
    "print(f\"rmse: {X1_rmse}\")\n",
    "\n",
    "X1_r2 = r2_score(y_true=y_test, y_pred=predictions)\n",
    "print(f\"r2 score: {X1_r2}\")"
   ],
   "metadata": {
    "id": "TCMOA02OeqPM",
    "outputId": "6a57723d-c5a4-4fc5-faea-356d1b3d4949"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### NN2 -- Higher Fidelity Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "75g2YTHVeqPN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "X_feats2 = ['release_speed', 'release_spin_rate', 'pfx_x', 'pfx_z', 'pfx_v', 'pitch_type_CH', 'pitch_type_CU', 'pitch_type_FC', 'pitch_type_FF', 'pitch_type_FS', 'pitch_type_KC', 'pitch_type_SI', 'pitch_type_SL', 'zone_1', 'zone_2', 'zone_3', 'zone_4', 'zone_5', 'zone_6', 'zone_7', 'zone_8', 'zone_9', 'zone_11', 'zone_12', 'zone_13', 'zone_14', 'p_throws_R']\n",
    "X_full = X_train[X_feats2]\n",
    "X_full_test = X_test[X_feats2]\n",
    "number_of_inputs = X_full.shape[1]  # number of features\n",
    "model = keras.Sequential([layers.Dense(3, activation='relu', input_dim=number_of_inputs),\n",
    "                          layers.Dense(3, activation='relu'),\n",
    "                          layers.Dense(1, activation='linear') ]) # output layer for regression\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), RSquare()])\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_full,\n",
    "    y_train,\n",
    "    batch_size=42,\n",
    "    epochs=100,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(X_full_test, y_test),\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_full_test, y_test, batch_size=26)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for test samples\")\n",
    "X2_predictions = model.predict(X_full_test)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "X2_rmse = mean_squared_error(y_true=y_test, y_pred=X2_predictions, squared=False)\n",
    "print(f\"rmse: {X2_rmse}\")\n",
    "\n",
    "X2_r2 = r2_score(y_true=y_test, y_pred=X2_predictions)\n",
    "print(f\"r2 score: {X2_r2}\")"
   ],
   "metadata": {
    "id": "TfuKL2MleqPN",
    "outputId": "42a3ac58-d9db-4075-a12c-6fd35f73ad22"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "X_feats3 = ['release_speed', 'release_spin_rate', 'pfx_x', 'pfx_z', 'pfx_v', 'pitch_type_CH', 'pitch_type_CU', 'pitch_type_FC', 'pitch_type_FF', 'pitch_type_FS', 'pitch_type_KC', 'pitch_type_SI', 'pitch_type_SL']\n",
    "X_full = X_train[X_feats3]\n",
    "X_full_test = X_test[X_feats3]\n",
    "number_of_inputs = X_full.shape[1]  # number of features\n",
    "model = keras.Sequential([layers.Dense(3, activation='relu', kernel_regularizer=regularizers.l1_l2(), input_dim=number_of_inputs),\n",
    "                          layers.Dense(3, activation='relu', kernel_regularizer=regularizers.l1_l2()),\n",
    "                          layers.Dense(1, activation='linear') ]) # output layer for regression\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), RSquare()])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_full,\n",
    "    y_train,\n",
    "    batch_size=42,\n",
    "    epochs=400,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(X_full_test, y_test),\n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_full_test, y_test, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for test samples\")\n",
    "X3_predictions = model.predict(X_full_test[X_feats3])\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "X3_rmse = mean_squared_error(y_true=y_test, y_pred=X3_predictions, squared=False)\n",
    "print(f\"rmse: {X3_rmse}\")\n",
    "\n",
    "X3_r2 = r2_score(y_true=y_test, y_pred=X3_predictions)\n",
    "print(f\"r2 score: {X3_r2}\")"
   ],
   "metadata": {
    "id": "WGQcArJAeqPN",
    "outputId": "65214ea7-8b91-4fe1-ef28-3d52ec4c0f6c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hyper sweep"
   ],
   "metadata": {
    "collapsed": false,
    "id": "GRJuMVxYeqPN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_and_compile_model():\n",
    "    # create model\n",
    "    model = keras.Sequential([layers.Dense(3, activation='relu', input_dim=number_of_inputs),\n",
    "                              layers.Dense(3, activation='relu'),\n",
    "                              layers.Dense(1, activation='linear')]) # output layer for regression\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse',\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_and_compile_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [8, 16, 32, 64, 128]\n",
    "epochs = [50, 100, 250, 500]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=5)\n",
    "\n",
    "grid_result = grid.fit(X_full, y_train)\n",
    "\n",
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')\n",
    "\n",
    "# Calling Method\n",
    "plot_grid_search(grid_result.cv_results_, batch_size, epochs, 'Batch Size', 'Epochs')"
   ],
   "metadata": {
    "id": "Oa7tgTCKeqPN",
    "outputId": "71ed2d3c-6c54-439c-e5e7-dc8d9d300090"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, param in zip(means, params):\n",
    "    print(\"train accuracy %f for: %r\" % (mean, param))"
   ],
   "metadata": {
    "id": "N0MGbe69eqPO",
    "outputId": "c5ef14b0-4b59-4c8e-afe4-6c3c51d8dba7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_and_compile_model(num_neurons, num_layers):\n",
    "    # create model\n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=number_of_inputs, activation='relu'))\n",
    "\n",
    "    if num_layers > 0:\n",
    "        while num_layers > 0:\n",
    "            model.add(Dense(num_neurons, activation='relu'))\n",
    "            num_layers=num_layers-1\n",
    "\n",
    "    model.add(Dense(1, activation='linear')) # output layer for regression\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse',\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), RSquare()])\n",
    "\n",
    "    print(model.summary())\n",
    "    print('--')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_and_compile_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "num_neurons_grid = [2, 3, 4, 5, 8, 10, 16]\n",
    "num_layers_grid = [1, 2, 3, 4]\n",
    "\n",
    "param_grid = dict(num_neurons=num_neurons_grid, num_layers=num_layers_grid)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=5)\n",
    "\n",
    "grid_result = grid.fit(X_full, y_train)"
   ],
   "metadata": {
    "id": "hNCHKDI2eqPO",
    "outputId": "86f1f710-d14d-484d-f061-9177a1a23a5c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')\n",
    "\n",
    "# Calling Method\n",
    "plot_grid_search(grid_result.cv_results_, num_neurons_grid, num_layers_grid, 'N Neurons', 'N Layers')\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, param in zip(means, params):\n",
    "    print(\"train accuracy %f for: %r\" % (mean, param))"
   ],
   "metadata": {
    "id": "ivLCP-mKeqPO",
    "outputId": "3fcdebf0-1f38-4bbd-9f73-b663b1a8bef3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Combining Sweep"
   ],
   "metadata": {
    "collapsed": false,
    "id": "dcDdbQIGeqPO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(10, activation='relu', input_dim=number_of_inputs, kernel_regularizer=regularizers.l1_l2()),\n",
    "                          layers.Dense(10, activation='sigmoid', kernel_regularizer=regularizers.l1_l2()),\n",
    "                          layers.Dense(1, activation='linear') ]) # output layer for regression\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), RSquare()])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_full,\n",
    "    y_train,\n",
    "    batch_size=8,\n",
    "    epochs=100,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(X_full_test, y_test),\n",
    "    callbacks=[callback],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_full_test, y_test, batch_size=8)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions\")\n",
    "predictions = model.predict(X_full_test)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "X4_rmse = mean_squared_error(y_true=y_test, y_pred=predictions, squared=False)\n",
    "print(f\"rmse: {X4_rmse}\")\n",
    "\n",
    "X4_r2 = r2_score(y_true=y_test, y_pred=predictions)\n",
    "print(f\"r2 score: {X4_r2}\")"
   ],
   "metadata": {
    "id": "HA6M9GnZeqPO",
    "outputId": "e832cdea-8e1e-4ce8-cfa0-00f0291f4f5c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TW NN"
   ],
   "metadata": {
    "id": "LK4GPXF6h81X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the loss plot\n",
    "def plot_loss(history, num_neurons):\n",
    "    plt.plot(history.history['root_mean_squared_error'], label='rmse')\n",
    "    plt.plot(history.history['val_root_mean_squared_error'], label='val_rmse')\n",
    "    # plt.ylim([0, y_lim])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mse')\n",
    "    plt.legend()\n",
    "    plt.title(f\"{num_neurons} neurons\")\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "from   tensorflow.keras import layers, regularizers\n",
    "from   tensorflow.keras.layers.experimental import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from   tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#setup normalizer\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "number_of_inputs= X_train.shape[1]\n",
    "print(f\"# of inputs = {number_of_inputs}\")\n",
    "\n",
    "num_neurons = 10\n",
    "\n",
    "# create model\n",
    "model = keras.Sequential([normalizer,\n",
    "                          layers.Dense(num_neurons, activation='relu', input_dim=number_of_inputs),\n",
    "                          layers.Dense(num_neurons, activation='sigmoid'),\n",
    "                          layers.Dense(1, activation='linear') ])  \n",
    "  \n",
    "model.compile(loss='mse',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), RSquare()])\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    batch_size=20,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0)  # Calculate validation results on 35% of the training data\n",
    "                  \n",
    "# Isolate and plot training curves \n",
    "hist = pd.DataFrame(history.history)\n",
    "plot_loss(history, num_neurons)\n",
    "\n",
    "# calculate metrics ---------------\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# train\n",
    "y_pred = model.predict(X_train)\n",
    "print(f\"MAE Train = {mean_absolute_error(y_train, y_pred)}\")\n",
    "print(f\"RMSE Train = {mean_squared_error(y_train, y_pred, squared=False)}\")\n",
    "print(f\"R2  = {                 r2_score(y_train, y_pred)}\")\n",
    "print(f\"\")\n",
    "\n",
    "# holdout\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"MAE Test = {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"RMSE Test = {mean_squared_error(y_test, y_pred, squared=False)}\")\n",
    "print(f\"R2  = {                r2_score(y_test, y_pred)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7W-1w75ceqPP",
    "outputId": "895b9f00-63a2-47ea-f495-9a999799e9b7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TW NN neurons sweep"
   ],
   "metadata": {
    "id": "0c2FEQCjiZIj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the loss plot\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['root_mean_squared_error'], label='rmse')\n",
    "    plt.plot(history.history['val_root_mean_squared_error'], label='val_rmse')\n",
    "    # plt.ylim([0, y_lim])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mse')\n",
    "    plt.legend()\n",
    "    plt.title(f\"{num_neurons} neurons\")\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "#setup normalizer\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "number_of_inputs = X_train.shape[1]\n",
    "\n",
    "# sweep neurons\n",
    "mse_list = []\n",
    "val_mse_list = []\n",
    "\n",
    "neurons_to_try = [1, 2, 3, 5, 7, 10, 15]\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "for num_neurons in neurons_to_try:\n",
    "\n",
    "    #---- pasted in model\n",
    "    model = keras.Sequential([normalizer,\n",
    "                            layers.Dense(num_neurons, activation='relu', input_dim=number_of_inputs),\n",
    "                            layers.Dense(num_neurons, activation='sigmoid'),\n",
    "                            layers.Dense(1, activation='linear') ])\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), RSquare()])\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=200,\n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        callbacks=[callback],\n",
    "                        verbose=0)  # Calculate validation results on 35% of the training data\n",
    "\n",
    "    # Isolate the metric for each epoch.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    #---- end model paste\n",
    "\n",
    "    # record metrics\n",
    "\n",
    "    current_mse = hist['root_mean_squared_error']\n",
    "    current_mse = current_mse[len(current_mse)-1]  # last element is the final value\n",
    "    mse_list.append(current_mse)\n",
    "\n",
    "    current_val_mse = hist['val_root_mean_squared_error']\n",
    "    current_val_mse = current_val_mse[len(current_val_mse)-1] #  last element is the final value\n",
    "    val_mse_list.append(current_val_mse)\n",
    "\n",
    "    plot_loss(history)\n",
    "\n",
    "    # calculate metrics ---------------\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    # train\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print(f\"MAE Train =     {mean_absolute_error(y_train, y_pred_train)}\")\n",
    "    print(f\"RMSE Train =    {mean_squared_error(y_train, y_pred_train, squared=False)}\")\n",
    "    print(f\"R2 =            {r2_score(y_train, y_pred_train)}\")\n",
    "    print(f\"\")\n",
    "\n",
    "    # holdout\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print(f\"MAE Test =      {mean_absolute_error(y_test, y_pred_test)}\")\n",
    "    print(f\"RMSE Test =     {mean_squared_error(y_test, y_pred_test, squared=False)}\")\n",
    "    print(f\"R2 =            {r2_score(y_test, y_pred_test)}\")\n",
    "\n",
    "#plot results of sweep\n",
    "plt.plot(neurons_to_try, mse_list, label='RMSE')\n",
    "plt.plot(neurons_to_try, val_mse_list, label='Validation RMSE')\n",
    "\n",
    "# plt.ylim([1,y_lim])\n",
    "plt.xlabel('Neurons')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VrbOdqLPia1K",
    "outputId": "9fb216d3-60ad-4ff1-e215-ac8ec1bb0623"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Final model on Fastball only\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# try groupby pitch_type\n",
    "# convert pitch dummies to single column\n",
    "org_pit_col_train = pd.from_dummies(X[['pitch_type_CH', 'pitch_type_CU', 'pitch_type_FC', 'pitch_type_FF',\n",
    "                                       'pitch_type_FS', 'pitch_type_KC', 'pitch_type_SI', 'pitch_type_SL']],\n",
    "                                    sep='pitch_type_')\n",
    "# drop dummy pitch_types from transformed train set\n",
    "X_grp_pit = X.drop(\n",
    "    columns=['pitch_type_CH', 'pitch_type_CU', 'pitch_type_FC', 'pitch_type_FF', 'pitch_type_FS', 'pitch_type_KC',\n",
    "             'pitch_type_SI', 'pitch_type_SL'], axis=1)\n",
    "# # add back zipped column, need to use .values attribute or will have index issues\n",
    "X_grp_pit['pitch_type'] = org_pit_col_train.values\n",
    "# # groupby pitch_type\n",
    "X_ff = X_grp_pit.groupby(['pitch_type']).get_group('FF')\n",
    "X_ff = X_ff.drop(columns=['pitch_type'])\n",
    "y = marte_vs.groupby(['pitch_type']).get_group('FF').xwOBA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ff, y, test_size=0.35, random_state=1642)\n",
    "\n",
    "print('Train set shape\\n', X_train.shape, y_train.shape)\n",
    "print('Test set shape\\n', X_test.shape, y_test.shape)\n",
    "print('---')\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "num_feats = ['release_speed', 'release_spin_rate', 'pfx_v', 'pfx_x', 'pfx_z']\n",
    "# used for positive values\n",
    "pt = PowerTransformer()\n",
    "nct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('pt_pre', pt, num_feats)\n",
    "    ])\n",
    "\n",
    "# get a transformed series of numerical features\n",
    "X_train_trx = pt.fit_transform(X_train[num_feats])\n",
    "# overwrite raw numerical data\n",
    "X_train[num_feats] = X_train_trx\n",
    "\n",
    "# transform the test set with the trained lambdas\n",
    "X_test_trx = pt.transform(X_test[num_feats])\n",
    "# overwrite raw numerical dataframe\n",
    "X_test[num_feats] = X_test_trx\n",
    "############################\n",
    "# # normalize output\n",
    "# mms = MinMaxScaler().fit(y_train.to_numpy().reshape(-1, 1))\n",
    "# y_train_trx = pd.Series(map(lambda x: x[0], mms.transform(y_train.to_numpy().reshape(-1, 1))), index=y_train.index)\n",
    "# # normalize on the test output, converted to a pandas.Series with the original indicies\n",
    "# y_test_trx = pd.Series(map(lambda x: x[0], mms.transform(y_test.to_numpy().reshape(-1, 1))), index=y_test.index)\n",
    "\n",
    "\n",
    "print('Train trx set shape\\n', X_train_trx.shape, y_train.shape)\n",
    "print('Test trx set shape\\n', X_test_trx.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#setup normalizer\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "model = keras.Sequential([normalizer,\n",
    "                          layers.Dense(10, activation='relu', input_dim=number_of_inputs, kernel_regularizer=regularizers.l1_l2()),\n",
    "                          layers.Dense(10, activation='sigmoid', kernel_regularizer=regularizers.l1_l2()),\n",
    "                          layers.Dense(1, activation='linear') ]) # output layer for regression\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), RSquare()])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=8,\n",
    "    epochs=100,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[callback],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test, batch_size=8)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions\")\n",
    "predictions = model.predict(X_test)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "X5_rmse = mean_squared_error(y_true=y_test, y_pred=predictions, squared=False)\n",
    "print(f\"rmse: {X5_rmse}\")\n",
    "\n",
    "X5_r2 = r2_score(y_true=y_test, y_pred=predictions)\n",
    "print(f\"r2 score: {X5_r2}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
